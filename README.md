<p align="center">
  <img src="samples/banner.png" alt="Music Generation with LSTM Banner" width="800"/>
</p>
# ğŸ¶ Music Generation with LSTM (Nottingham Dataset)

This project implements an **LSTM-based symbolic music generation system** trained on the [Nottingham Music Dataset](https://www.kaggle.com/datasets/eftimiepetre/nottingham-music).  
It demonstrates an end-to-end workflow: **data preparation â†’ model training â†’ evaluation â†’ autoregressive music generation â†’ MIDI export**.  
The final system generates polyphonic piano-roll sequences and exports them as MIDI files for playback in DAWs or sheet-music editors like MuseScore.

---

## ğŸ“‚ Project Structure
```
music-generation-lstm/
â”œâ”€â”€ data/                  # Dataset folder (ignored, see data/README.md for download)
â”‚   â””â”€â”€ README.md          # Instructions to download Nottingham.mat
â”œâ”€â”€ models/                # Saved weights
â”‚   â””â”€â”€ lstm_nottingham.pt # Pretrained model (~2.9 MB included for demo)
â”œâ”€â”€ notebooks/             # Kaggle/Colab experiments
â”œâ”€â”€ samples/               # Demo outputs (kept small)
â”‚   â”œâ”€â”€ lstm_sample.mid    # Example generated music
â”‚   â”œâ”€â”€ evaluation.png     # Pitch histogram plot
â”‚   â””â”€â”€ train_val_loss.png     # Training/validation loss curve
â”œâ”€â”€ src/                   # Core source code
â”‚   â”œâ”€â”€ dataset.py         # .mat â†’ piano-roll preprocessing, Dataset/DataLoader
â”‚   â”œâ”€â”€ model.py           # MusicLSTM definition
â”‚   â”œâ”€â”€ train.py           # Training loop (AMP, clipping, early stopping)
â”‚   â”œâ”€â”€ generate.py        # Autoregressive generation + MIDI export
â”‚   â”œâ”€â”€ evaluate.py        # Quantitative checks (metrics, density, pitch histogram)
â”‚   â””â”€â”€ utils.py           # Precision/Recall, density, histogram helpers
â”œâ”€â”€ .gitignore             # Ignore data/, temp samples, extra weights
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # Project documentation
```

---

## âš™ï¸ Installation
Clone and install dependencies:
```bash
git clone https://github.com/your-username/music-generation-lstm.git
cd music-generation-lstm
pip install -r requirements.txt
```

Requirements:
- Python â‰¥ 3.9  
- PyTorch â‰¥ 2.1 (with CUDA for GPU acceleration)  

---

## ğŸ“Š Dataset
We do **not** include the full Nottingham dataset in this repo (too large).  
Instead, download the `.mat` file from Kaggle and place it under `data/`:

```
data/Nottingham.mat
```

See `data/README.md` for detailed instructions.

---

## ğŸš€ Usage

### 1. Train the model
```bash
python -m src.train --data data/Nottingham.mat --epochs 20 --seq-len 128 --batch-size 32 --lr 1e-5 --save models/lstm_nottingham.pt
```

### 2. Generate music
```bash
python -m src.generate --weights models/lstm_nottingham.pt --data data/Nottingham.mat --seed-index 0 --seed-len 192 --steps 512 --temperature 1.0 --threshold 0.5 --out samples/lstm_sample.mid
```

This creates a `.mid` file playable in **MuseScore**, **FL Studio**, or any DAW.

### 3. Evaluate the model
```bash
python -m src.evaluate --weights models/lstm_nottingham.pt --data data/Nottingham.mat --seq-len 128 --batch-size 32 --seed-index 0 --seed-len 192 --steps 512 --temperature 1.0 --threshold 0.50 --plot samples/evaluation.png
```

Youâ€™ll get:
- **Precision / Recall / F1** on validation  
- **Note density** (real vs generated)  
- **Pitch histogram plot** (`samples/evaluation.png`)  

---

## ğŸ“ˆ Example Results
- Best validation loss â‰ˆ **0.25** after 20 epochs (on Kaggle GPU).  
- **F1-score ~0.56** at threshold 0.5.  
- Generated samples sound musically coherent but slightly denser (tend to over-predict notes).  

See included demo files in `samples/`:  
- ğŸµ `lstm_sample.mid` â†’ Generated music example  
- ğŸ“Š `evaluation.png` â†’ Pitch distribution comparison  
- ğŸ“‰ `train_val_loss_.png` â†’ Training/validation loss curve /  

---

## ğŸ› ï¸ Key Features
- End-to-end pipeline (data â†’ training â†’ generation â†’ evaluation).  
- Reproducible with CLI scripts.  
- GPU-accelerated training (AMP, gradient clipping, early stopping).  
- Quantitative + qualitative evaluation (metrics + MIDI + plots).  
- One pretrained LSTM model (`models/lstm_nottingham.pt`) and demo outputs included.  

---

## ğŸ“Œ Next Steps
- Experiment with **Transformer-based architectures** for comparison.  
- Tune thresholds, temperature, and sequence length.  
- Fine-tune on custom MIDI datasets.  

---

## ğŸ“œ License
MIT License.  Feel free to use, modify, and build upon this project.

---